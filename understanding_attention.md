> 笔记部分内容来自书《神经网络与深度学习》——邱锡鹏（讲的很清晰😭）
## 从人脑中的借鉴

注意力是一种人类不可或缺的复杂认知功能，我们能从大量外界信息轰炸中有条不紊的工作，就是因为人脑可以有意无意地从这些大量输入信息中选择小部分有用信息处理利用，并忽略其他信息，这种能力就叫**注意力**。其一般可分为两种：
1. 自上而下的有意识注意力，称为**聚焦式注意力**，其具有预定目的或依赖任务，主动有意识地聚焦于某一对象
2. 自上而下的无意识的注意力，称为**基于显著性的注意力**，其由外界刺激驱动，不需要主动干预，也和任务无关

在目前的神经网络模型中，我们可以将**最大汇聚**（Max Pooling），**门控机制**（Gating）近似地看作自下而上的基于显著性的注意力机制
> 之前看过有说法说像CNN，RNN这样的神经网络是带有比较多人类的先验知识来设计的，所以其在数据量相对较少的情况下其在各自领域具有优秀表现，而注意力机制的设计更具有通用性，在更大数据量时表现出更加优异的性能

现在我们用 $X = [x_1,...,x_n]$ 表示 $N$ 组输入信息，其中 $x_n$ 表示 一个 $D$ 维向量，则 $X$ 表示一个向量组，使用注意力进行任务时，我们往往也只需要 $X$ 的一部分相关信息，注意力的计算可以分为两部分：
1. 在所有输入信息上计算注意力分布
2. 根据注意力分布来计算输入信息的加权平均

## 注意力分布

为了从 $N$ 个向量 $[x_1,...,x_n]$ 中选择出和某特定任务相关的信息，我们需要引入一个和任务相关的表示，将其称为**查询向量**（**Query** Vector），并通过一个打分函数来计算**每个输入向量**和**查询向量**之间的**相关性**

给定一个和任务相关的查询向量 $q$ ，我们用注意力变量 $z \in [1,N]$ 来表示被选择变量的索引位置，即 $z=n$ 表示选择了第 $n$ 个输入向量，为计算方便这里采用一种 “软性” 的信息选择机制，首先计算在给定 $q$ 和 $X$ 下，选择第 $n$ 个向量的概率 $\alpha_n$

$$
\begin{align*}
\alpha_n 
&=p(z=n|X,q) \\
~\\
&=softmax(s(x_n,q)) \\
~\\
&=\frac{exp(s(x_n,q))}{\sum_{i=1}^{n}exp(s(x_j,q))}
\end{align*}
$$

其中 $\alpha_n$ 称为注意力分布（Attention Distribution），$s(x,q)$ 称为打分函数，可以使用以下几种方式实现：
1. 加性模型

$$
s(x,q)=v^Ttanh(Wx+Uq)
$$

其中 $W，U，v$ 都是可学习的参数，在Pytorch实现中通常使用 `nn.Linear` 实现

其首次提出是在论文[https://arxiv.org/abs/1409.0473v7](https://arxiv.org/abs/1409.0473v7)中，在一个 `Seq2Seq` 模型中加入了注意力机制

相关介绍与代码实现可见：[https://blog.csdn.net/2303_76215922/article/details/144089266](https://blog.csdn.net/2303_76215922/article/details/144089266)

2. 点积模型

$$
s(x,q)=x^Tq
$$

3. **缩放点积模型**

$$
s(x,q)=\frac{x^Tq}{\sqrt{D}}
$$

其中 $D$ 是输入向量的维度
4. 双线性模型

$$
s(x,q)=x^TWq
$$

双线性模型是一种**泛化的点积模型**，假设公式中 $W=U^TV$，则公式可写为 

$$
s(x,q)=x^TU^TVq=(Ux)^T(Vq)
$$

即分别对 $x$ 和 $q$ 进行线性变换后计算点积，在计算相似度时引入了非线性

理论上加性和点积的计算复杂度差不多，但是点积模型在实现上可以更好利用矩阵乘法，从而计算效率更高

当输入维度 $D$ 较大时，点积模型的值通常具有较大的方差，从而导致 $Softmax$ 函数的梯度较小，因此，使用**缩放点积注意力**可以较好的解决这个问题

**进一步说明：**

>给定两个向量 $\mathbf{q}$（查询向量）和 $\mathbf{k}$（键向量），它们的点积定义为：

$$
\mathbf{q} \cdot \mathbf{k} = \sum_{i=1}^{D} q_i k_i
$$

假设向量 $\mathbf{q}$ 和 $\mathbf{k}$ 的各维度元素 $q_i$ 和 $k_i$ 是独立同分布的随机变量，且均值为零，方差为 $\sigma^2$。则点积 $\mathbf{q} \cdot \mathbf{k}$ 的期望和方差分别为：

$$
\mathbb{E}[\mathbf{q} \cdot \mathbf{k}] = 0
$$

$$
\text{Var}(\mathbf{q} \cdot \mathbf{k}) = \sum_{i=1}^{D} \text{Var}(q_i k_i) = D \cdot \sigma^4
$$

这表明，随着维度 $D$ 的增加，点积的方差 $\text{Var}(\mathbf{q} \cdot \mathbf{k})$ 线性增长。因此，在高维空间中，点积值的分布会变得更加分散，值的范围也会更大。
给定一组未归一化的得分 $\mathbf{z} = [z_1, z_2, \dots, z_N]$，Softmax 函数将其转换为概率分布：

$$
\text{Softmax}(z_i) = \frac{\exp(z_i)}{\sum_{j=1}^{N} \exp(z_j)}
$$

当 $D$ 较大时，点积 $\mathbf{q} \cdot \mathbf{k}$ 的方差增大，意味着不同 $z_i$ 之间的差异也会增大。这导致：
>- **概率分布更加尖锐**：某些 $z_i$ 会远大于其他 $z_j$，使得 $\text{Softmax}(z_i)$ 接近于 1，而其他概率接近于 0。
>- **梯度消失**：由 $\text{Softmax}$ 的图像可知，当其值接近1或0的时候，其梯度会变得非常小，这会导致反向传播时，梯度信号变弱，影响模型的学习效果。

## 加权平均

注意力分布 $\alpha_n$ 可以解释为在给定任务相关的查询 $\mathbf{q}$ 时，第 $n$ 个输入向量受关注的程度，我们采用一种软性的信息选择机制对输入信息进行汇总（软注意力）即：

$$
attn(X,q)=\sum_{n=1}^{N}\alpha_nx_n
$$

上述公式称为**软性注意力机制**，注意力机制可以单独使用，但更多作为神经网络中的一个组件

## 注意力机制的变体

### 硬性注意力

其只关注某一个输入向量，有两种实现方式：
1. 选取概率最高的一个输入向量，即

$$
attn(x,q)=x_{\hat{n}}
$$

其中 $\hat{n}$ 为概率最大的输入向量的下标，即 $\arg\max_{n=1}^{N} \alpha_n$

2. 通过在注意力分布上随机采样的方式实现

硬注意力的一个缺点是其基于最大采样或者随机采样的方式，使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算法进行训练，通常需要强化学习来对其进行训练

### 键值对注意力

更一般地，我们可以使用 **键值对** 来表示输入信息，其中 **键** 用来计算注意力分布 $\alpha_n$ ，**值** 用来计算聚合信息

用 $(\mathbf{K,V})=[(k_1,v_1),...(k_n,v_n)]$ 来表示 $N$ 组输入信息，给定查询向量 $q$ 时，注意力函数为：

$$
\begin{align*}
attn((\mathbf{K,V}),q) &=\sum_{n=1}^{N}\alpha_n\pmb{v_n} \\
&= \sum_{n=1}^{N}\frac{exp(s(k_n,q))}{\sum_{j}exp(s(k_j,q))}\pmb{v_n}
\end{align*}
$$

其中 $s\left( k_n,q\right)$ 为打分函数，从前面介绍注意力时也可以看出

$$
attn(X,q)=\sum_{n=1}^{N}\alpha_nx_n
$$

当 $K=V$ 时，键值对注意力也就变成了如上的普通注意力， $x_n$ 就表示了 $k$ 和 $v$ 

### 多头注意力

多头注意力（Multi-Head Attention）是利用多个查询 $Q=[q_1,...q_m]$ 来并行地从输入信息中选取多组信息，每个头关注输入信息不同的部分

$$
attn\left( \left( K,V\right),Q\right)=attn\left( \left( K,V\right),q_1\right) \oplus ... \oplus attn\left( \left( K,V\right),q_m\right)
$$

其中 $\oplus$ 表示向量拼接
